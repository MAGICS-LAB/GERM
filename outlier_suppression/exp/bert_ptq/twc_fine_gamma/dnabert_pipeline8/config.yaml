quant: 
    is_remove_padding: True
    calibrate: 256
    ln: 
        delay: True
    a_qconfig:
        quantizer: FixedFakeQuantize 
        observer: AvgMinMaxObserver 
        bit: 8
        symmetric: False
        ch_axis: -1
    w_qconfig:
        quantizer: FixedFakeQuantize
        observer: MinMaxObserver
        bit: 8
        symmetric: True
        ch_axis: 0
data:
    task_name: dnabert
    dataset_name: null
    is_regression: False # if stsb, use True
    dataset_config_name: null
    max_seq_length: 128
    overwrite_cache: False # Overwrite the cached preprocessed datasets or not.
    pad_to_max_length: True # Whether to pad all samples to 'max_seq_length'
                            # If False, will pad the samples dynamically when batching to the maximum length in the batch."
    max_train_samples: null
    max_eval_samples: null
    max_predict_samples: null
    train_file: /scratch/hlv8980/data/covid/train.csv
    validation_file: /scratch/hlv8980/data/covid/dev.csv
    test_file: /scratch/hlv8980/data/covid/test.csv
    streaming: False
    preprocessing_num_workers: 16
    overwrite_cache: False
    line_by_line: True
    validation_split_percentage: 5
    kmer: -1

model:
    model_type: bert
    model_name_or_path: /scratch/hlv8980/FT_result/output_van160+40_Full_double/van160+40_dnabert2_Full_double_covid
    config_name: /projects/p32013/DNA/DNABERT2 # pretrained config name or path if not the same as model_name
    tokenizer_name: /projects/p32013/DNA/DNABERT2
    use_slow_tokenizer: False
    token: None
    trust_remote_code: True
    cache_dir: /projects/p32013/DNA/outlier_suppression/cache #Where do you want to store the pretrained models downloaded from huggingface.co
    use_fast_tokenizer: True # whether to use one of the fast tokenizer (backed by the tokenizers library) or not
    model_revision: main # The specific model version to use (can be a branch name, tag name or commit id).
    use_auth_token: Fasle # will use the token generated when running `transformers-cli login` (necessary to use this script "
            # with private models)"
    

train:
    seed: 3407
    output_dir: /scratch/hlv8980/redo2/validate_outsup8/van160+40/covid
    overwrite_output_dir: /scratch/hlv8980/redo2/validate_outsup8/van160+40/covid
    do_train: Flase
    do_eval: True
    do_predict: False
    evaluation_strategy: "no" #The evaluation strategy to use. "no"; "steps"; "epoch"
    eval_steps: 500 # Run an evaluation every X steps.
    per_device_train_batch_size: 32 # Batch size per GPU/TPU core/CPU for training.
    per_device_eval_batch_size: 32 # Batch size per GPU/TPU core/CPU for evaluation
    gradient_accumulation_steps: 1 # Number of updates steps to accumulate before performing a backward/update pass.
    eval_accumulation_steps: null
    learning_rate: 5.0e-5 # The initial learning rate for AdamW.
    weight_decay: 0.0 # Weight decay for AdamW if we apply some.
    max_grad_norm: 1.0 # Max gradient norm.
    num_train_epochs: 3.0 #Total number of training epochs to perform.
    max_steps: 10000  # If > 0: set total number of training steps to perform. Override num_train_epochs.
    lr_scheduler_type: linear # The scheduler type to use.
    warmup_ratio: 0.0 # Linear warmup over warmup_ratio fraction of total steps.
    warmup_steps: 0 # Linear warmup over warmup_steps.
    gradient_checkpointing: False  # If True, use gradient checkpointing to save memory at the expense of slower backward pass.


progress:
    log_level: passive # Logger log level to use on the main node. Possible choices are the log levels as strings: 'debug', 'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the application set the level. Defaults to 'passive'.
    log_level_replica: passive # Logger log level to use on replica nodes.
    logging_dir: null # Tensorboard log dir.
    logging_strategy: steps # The logging strategy to use. "no"; "steps"; "epoch";
    logging_steps: 1000 # Log every X updates steps.
    
    save_strategy: "no" # The checkpoint save strategy to use. "no"; "steps"; "epoch";
    save_steps: 1000 # Save checkpoint every X updates steps.
    save_total_limit: null # Limit the total amount of checkpoints.
                           # Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints
    save_on_each_node: False #When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on the main one
    
    no_cuda: False # Do not use CUDA even when it is available
    run_name: null # An optional descriptor for the run. Notably used for wandb logging.
    disable_tqdm: null # Whether or not to disable the tqdm progress bars. use False or True
    
    load_best_model_at_end: False  #Whether or not to load the best model found during training at the end of training.
    metric_for_best_model: null # The metric to use to compare two different models."
    greater_is_better: null # Whether the `metric_for_best_model` should be maximized or not.
